{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmistry1/KNN-Classifier/blob/main/Knn_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PpbEMaXe3nb7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY-Ego20lK3g"
      },
      "source": [
        "# Problem 1:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "p9WkA9h7hF1k",
        "outputId": "45fc021e-0491-46ff-91ae-e93038150d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "N = 3000, Accuracy = 0.7843, Runtime = 2.87 seconds\n",
            "N = 5000, Accuracy = 0.8020, Runtime = 5.21 seconds\n"
          ]
        }
      ],
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Function to train and evaluate KNN classifier\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, n_neighbors):\n",
        "    start_time = time.time()\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    end_time = time.time()\n",
        "    runtime = end_time - start_time\n",
        "    return accuracy, runtime\n",
        "\n",
        "# Define different values of N\n",
        "# N_values = [2000, 4000]\n",
        "N_values = [3000, 5000]\n",
        "# N_values = [1500, 3000]\n",
        "\n",
        "# Empty lists to store results\n",
        "accuracies = []\n",
        "runtimes = []\n",
        "\n",
        "# Iterate over N_values\n",
        "for N in N_values:\n",
        "    # Subset the training set\n",
        "    X_train_subset = []\n",
        "    y_train_subset = []\n",
        "    for i in range(10):  # Fashion MNIST has 10 classes\n",
        "        X_class = X_train[y_train == i][:N//10]\n",
        "        y_class = y_train[y_train == i][:N//10]\n",
        "        X_train_subset.extend(X_class)\n",
        "        y_train_subset.extend(y_class)\n",
        "    X_train_subset = np.array(X_train_subset)\n",
        "    y_train_subset = np.array(y_train_subset)\n",
        "\n",
        "    # Train and evaluate KNN\n",
        "    accuracy, runtime = train_and_evaluate(X_train_subset.reshape(-1, 784), X_test.reshape(-1, 784), y_train_subset, y_test, n_neighbors=5)\n",
        "\n",
        "    # Append results\n",
        "    accuracies.append(accuracy)\n",
        "    runtimes.append(runtime)\n",
        "\n",
        "# Print results\n",
        "for N, accuracy, runtime in zip(N_values, accuracies, runtimes):\n",
        "    print(f\"N = {N}, Accuracy = {accuracy:.4f}, Runtime = {runtime:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVHazQO3pMui"
      },
      "source": [
        "In the code block above, I have tests these followig values. Here are my observations.\n",
        "\n",
        "**N1=2000, N2=4000**\n",
        "```\n",
        "N = 2000, Accuracy = 0.7765, Runtime = 2.05 seconds\n",
        "N = 4000, Accuracy = 0.7946, Runtime = 4.48 seconds\n",
        "```\n",
        "\n",
        "These values is a good balance between efficiency and model performance.\n",
        "\n",
        "N1 = 2000 for each class would provide a reasonably sized training set for each class without overwhelming computational resources.\n",
        "\n",
        "N2 = 4000 for each class could provide a larger dataset for better model performance without excessively increasing runtime.\n",
        "\n",
        "**N1=3000, N2=5000**\n",
        "```\n",
        "N = 3000, Accuracy = 0.7843, Runtime = 3.94 seconds\n",
        "N = 5000, Accuracy = 0.8020, Runtime = 4.13 seconds\n",
        "```\n",
        "\n",
        "These values lean towards better model performance at the cost of increased runtime.\n",
        "\n",
        "N1 = 3000 for each class provides more data for training without a significant increase in runtime.\n",
        "\n",
        "N2 = 5000 for each class offers even more data for potentially better model generalization.\n",
        "\n",
        "**N1=1500, N2=3000**\n",
        "```\n",
        "N = 1500, Accuracy = 0.7669, Runtime = 1.76 seconds\n",
        "N = 3000, Accuracy = 0.7843, Runtime = 2.54 seconds\n",
        "```\n",
        "\n",
        "These values prioritize computational efficiency while still providing a reasonable amount of data for training.\n",
        "\n",
        "N1 = 1500 for each class reduces computational overhead while still allowing the model to learn from a diverse set of examples.\n",
        "\n",
        "N2 = 3000 for each class balances computational efficiency with improved model performance compared to smaller training sets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rAbBAvj61fd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emm0Ecc965FF"
      },
      "source": [
        "# Problem 2a:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm that we will do is the following:\n",
        "- For each class in Fashion MIST:\n",
        " - Get the indices of samples belonging to that class.\n",
        " - Randomly shuffle the indices.\n",
        " - Select the first N1 indices for the training set.\n",
        " - Select the remaining samples (7000 - N1) for the testing set.\n",
        "- Repeat the process for all classes."
      ],
      "metadata": {
        "id": "RyOk7wUuLp-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-jJjo7-W7MVZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e80f7659-9632-4356-abf1-09f32f27a058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_split: (40000, 28, 28)\n",
            "Shape of X_test_split: (20000, 28, 28)\n",
            "Shape of y_train_split: (40000,)\n",
            "Shape of y_test_split: (20000,)\n"
          ]
        }
      ],
      "source": [
        "# Define N1\n",
        "N1 = 4000\n",
        "\n",
        "# Initialize lists to store training and testing data\n",
        "X_train_split = []\n",
        "X_test_split = []\n",
        "y_train_split = []\n",
        "y_test_split = []\n",
        "\n",
        "# Iterate over each class\n",
        "for class_label in range(10):  # Fashion MNIST has 10 classes\n",
        "    # Get indices of samples belonging to the current class\n",
        "    class_indices = np.where(y_train == class_label)[0]\n",
        "\n",
        "    # Shuffle indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Select first N1 indices for training set\n",
        "    train_indices = class_indices[:N1]\n",
        "    X_train_split.extend(X_train[train_indices])\n",
        "    y_train_split.extend(y_train[train_indices])\n",
        "\n",
        "    # Select remaining samples for testing set\n",
        "    test_indices = class_indices[N1:7000]  # 7000 - N1\n",
        "    X_test_split.extend(X_train[test_indices])\n",
        "    y_test_split.extend(y_train[test_indices])\n",
        "\n",
        "# Converting the lists to numpy arrays\n",
        "X_train_split = np.array(X_train_split)\n",
        "X_test_split = np.array(X_test_split)\n",
        "y_train_split = np.array(y_train_split)\n",
        "y_test_split = np.array(y_test_split)\n",
        "\n",
        "# Print shapes of resulting arrays\n",
        "print(\"Shape of X_train_split:\", X_train_split.shape)\n",
        "print(\"Shape of X_test_split:\", X_test_split.shape)\n",
        "print(\"Shape of y_train_split:\", y_train_split.shape)\n",
        "print(\"Shape of y_test_split:\", y_test_split.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2b:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UOknyCf7Mqnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the images\n",
        "X_train_flat = X_train_split.reshape(-1, 784)\n",
        "X_test_flat = X_test_split.reshape(-1, 784)\n",
        "\n",
        "# Define fixed k value\n",
        "k = 20\n",
        "\n",
        "# Train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train_flat, y_train_split)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn.predict(X_test_flat)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_split, y_pred)\n",
        "print(f\"Global success rate with N1 = {N1}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DlF3QKF8PQcr",
        "outputId": "cbdf773d-eec8-46e5-c33b-e0c87170e5e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global success rate with N1 = 4000: 0.8404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2c:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9BhOWbnzYw1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define values of N1, N2, and N3\n",
        "N_values = [3000, 4000, 5000]\n",
        "\n",
        "# Define fixed k value\n",
        "k = 20\n",
        "\n",
        "# Initialize empty lists to store accuracies\n",
        "accuracies_N = []\n",
        "\n",
        "# Iterate over each value of N1, N2, and N3\n",
        "for N in N_values:\n",
        "    # Initialize lists to store training and testing data\n",
        "    X_train_split = []\n",
        "    X_test_split = []\n",
        "    y_train_split = []\n",
        "    y_test_split = []\n",
        "\n",
        "    # Iterate over each class\n",
        "    for class_label in range(10):  # Fashion MNIST has 10 classes\n",
        "        # Get indices of samples belonging to the current class\n",
        "        class_indices = np.where(y_train == class_label)[0]\n",
        "\n",
        "        # Shuffle indices\n",
        "        np.random.shuffle(class_indices)\n",
        "\n",
        "        # Select first N indices for training set\n",
        "        train_indices = class_indices[:N]\n",
        "        X_train_split.extend(X_train[train_indices])\n",
        "        y_train_split.extend(y_train[train_indices])\n",
        "\n",
        "        # Select remaining samples for testing set\n",
        "        test_indices = class_indices[N:7000]  # 7000 - N\n",
        "        X_test_split.extend(X_train[test_indices])\n",
        "        y_test_split.extend(y_train[test_indices])\n",
        "\n",
        "    # Convert the lists to numpy arrays\n",
        "    X_train_split = np.array(X_train_split)\n",
        "    X_test_split = np.array(X_test_split)\n",
        "    y_train_split = np.array(y_train_split)\n",
        "    y_test_split = np.array(y_test_split)\n",
        "\n",
        "    # Flatten the images\n",
        "    X_train_flat = X_train_split.reshape(-1, 784)\n",
        "    X_test_flat = X_test_split.reshape(-1, 784)\n",
        "\n",
        "    # Train the KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_flat, y_train_split)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = knn.predict(X_test_flat)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test_split, y_pred)\n",
        "    accuracies_N.append(accuracy)\n",
        "\n",
        "    print(f\"N = {N}, Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "H5lLr34FTEJO",
        "outputId": "138e6b38-9d9f-49b8-ebf3-0452b295fc68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 3000, Accuracy = 0.8367\n",
            "N = 4000, Accuracy = 0.8407\n",
            "N = 5000, Accuracy = 0.8478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, I use 3000, 4000 and 5000 for N and we can seee that with 3000 the accuracy was 83.59%, with 4000 the accuracy was 84.37% and with 5000 the accuracy was 84.45%.\n",
        "\n",
        "Based on these results, we can see as the size of the training set increased from 3000 to 5000, the accuracy of the classification improves, which means, having more training data tends to lead to better classification performance.\n",
        "\n",
        "We can also notice the increase in accuracy from 4000 to 5000 is smaller compared to the increase from 3000 to 4000. This suggests that there might be diminishing returns in terms of accuracy improvement as we further increase the training set size.\n",
        "\n",
        "To choose an optimal size for the training set, we can take the cross validation approach. We split the dataset into multiple training and testing sets, train the model on each training set, and evaluate its performance on the corresponding testing set. Repeat this process for different sizes of the training set and monitor the model's performance metrics  for each size. This This approach will make sure that the chosen size generalizes well to unseen data and helps in avoiding overfitting or underfitting.\n",
        "\n",
        "\n",
        "The notion of optimality in this context can be defined as achieving the highest possible accuracy on unseen data while considering computational constraints and avoiding overfitting. By using cross-validation to assess performance across different training set sizes, we can select the size that strikes the best balance between model performance and generalization."
      ],
      "metadata": {
        "id": "xnYz1wcdWDlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3a:\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "X5HkdkoyaB1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New algorithm:\n",
        "- Instead of selecting the first N1 samples for the training set, let's try tto create an evenly distributed training set.\n",
        "- Then we will make sure that each class is repreesented proportionally in the training set. I feel like this will help prevent biad towards certain classes.\n",
        "- After that we within each class, wee will randomly select N1 samples for the training set."
      ],
      "metadata": {
        "id": "oxaTkbhQaQXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Define N1\n",
        "N1 = 4000  # or any other value between 1000 and 6000\n",
        "\n",
        "# Initialize lists to store training and testing data\n",
        "X_train_split = []\n",
        "X_test_split = []\n",
        "y_train_split = []\n",
        "y_test_split = []\n",
        "\n",
        "# Iterate over each class\n",
        "for class_label in range(10):  # Fashion MNIST has 10 classes\n",
        "    # Get indices of samples belonging to the current class\n",
        "    class_indices = np.where(y_train == class_label)[0]\n",
        "\n",
        "    # Randomly select N1 indices for the training set using stratified sampling\n",
        "    train_indices, _ = train_test_split(class_indices, train_size=N1, stratify=y_train[class_indices])\n",
        "\n",
        "    # Append selected training samples to the training data list\n",
        "    X_train_split.extend(X_train[train_indices])\n",
        "    y_train_split.extend(y_train[train_indices])\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train_split = np.array(X_train_split)\n",
        "y_train_split = np.array(y_train_split)\n",
        "\n",
        "# Testing set is automatically formed from the remaining data\n",
        "X_test_split = np.delete(X_train, train_indices, axis=0)\n",
        "y_test_split = np.delete(y_train, train_indices)\n",
        "\n",
        "# Print shapes of resulting arrays\n",
        "print(\"Shape of X_train_split:\", X_train_split.shape)\n",
        "print(\"Shape of X_test_split:\", X_test_split.shape)\n",
        "print(\"Shape of y_train_split:\", y_train_split.shape)\n",
        "print(\"Shape of y_test_split:\", y_test_split.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CI9yLpirbRZ5",
        "outputId": "3f417171-5a50-4bf3-c005-6bf14d75f434"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_split: (40000, 28, 28)\n",
            "Shape of X_test_split: (56000, 28, 28)\n",
            "Shape of y_train_split: (40000,)\n",
            "Shape of y_test_split: (56000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3b:\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tfzCzuAQPTfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Define N\n",
        "N = 4000  # or any other value between 1000 and 6000\n",
        "\n",
        "# Initialize lists to store training and testing data for each class\n",
        "X_train_split = []\n",
        "X_test_split = []\n",
        "y_train_split = []\n",
        "y_test_split = []\n",
        "\n",
        "# Iterate over each class\n",
        "for class_label in range(10):  # Fashion MNIST has 10 classes\n",
        "    # Get indices of samples belonging to the current class\n",
        "    class_indices = np.where(y_train == class_label)[0]\n",
        "\n",
        "    # Randomly select N indices for the training set using stratified sampling\n",
        "    train_indices, _ = train_test_split(class_indices, train_size=N, stratify=y_train[class_indices])\n",
        "\n",
        "    # Append selected training samples to the training data list\n",
        "    X_train_split.extend(X_train[train_indices])\n",
        "    y_train_split.extend(y_train[train_indices])\n",
        "\n",
        "    test_indices = np.setdiff1d(class_indices, train_indices)\n",
        "    X_test_split.extend(X_train[test_indices])\n",
        "    y_test_split.extend(y_train[test_indices])\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train_split = np.array(X_train_split)\n",
        "X_test_split = np.array(X_test_split)\n",
        "y_train_split = np.array(y_train_split)\n",
        "y_test_split = np.array(y_test_split)\n",
        "\n",
        "# Print shapes of resulting arrays\n",
        "print(\"Shape of X_train_split:\", X_train_split.shape)\n",
        "print(\"Shape of X_test_split:\", X_test_split.shape)\n",
        "print(\"Shape of y_train_split:\", y_train_split.shape)\n",
        "print(\"Shape of y_test_split:\", y_test_split.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "olhx7dy_b3Ot",
        "outputId": "8c63f4b4-bc9e-436c-958b-a81d336005e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_split: (40000, 28, 28)\n",
            "Shape of X_test_split: (20000, 28, 28)\n",
            "Shape of y_train_split: (40000,)\n",
            "Shape of y_test_split: (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3c\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "O88AIVaHPolu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create KNN classifier with k = 20\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=20)\n",
        "\n",
        "# Flatten the images for KNN classifier\n",
        "X_train_flatten = X_train_split.reshape(len(X_train_split), -1)\n",
        "X_test_flatten = X_test_split.reshape(len(X_test_split), -1)\n",
        "\n",
        "# Train the KNN classifier\n",
        "knn_classifier.fit(X_train_flatten, y_train_split)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = knn_classifier.predict(X_test_flatten)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_split, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QLB6PNe5cu3t",
        "outputId": "a2ac919e-7d82-4c2e-9d19-5abe5cfda328"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3d\n",
        "\n",
        "---\n",
        "\n",
        "With this new algorithm we are getting accuracy of 0.841, it appears that the new split method, where each class has a training set consisting of N examples and a testing set consisting of 7000 - N examples, has yielded a reasonably high accuracy for the KNN classification task."
      ],
      "metadata": {
        "id": "m46ZVUXjOL_A"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWW7iGXDCte479dcTzeyW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}